{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38888632-107c-4035-b5f5-35d6aff35ffe",
   "metadata": {},
   "source": [
    "# 패키지 설치 (python 3.8.10 기준)\n",
    "아래 두 가지 방법 중 한 가지를 택하여 설치합니다.\n",
    "\n",
    "## pip 사용\n",
    "```pip install -r requirements.txt```\n",
    "\n",
    "## conda 환경 사용 \n",
    "```conda env create -f environment.yml```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb31c3c-4cc2-4d86-9d8e-d305ec8a6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "#!conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97d496-ee40-4e48-80a5-7baa4b90d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LAVIS (BLIP 모델용)\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "# Diffusers 및 Pix2Pix-Zero 유틸리티\n",
    "from diffusers import DDIMScheduler\n",
    "from src.utils.ddim_inv import DDIMInversion\n",
    "from src.utils.scheduler import DDIMInverseScheduler\n",
    "from src.utils.edit_pipeline import EditingPipeline\n",
    "from src.utils.edit_directions import construct_direction\n",
    "\n",
    "# 전역 변수 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db9349-791d-4fb8-aa5c-9e2d99060cd4",
   "metadata": {},
   "source": [
    "## Direction for prompt embedding edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892b196-0000-49bf-ab65-f500d6560fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_direction(task_name):\n",
    "    # src2dst 에서 src 와 dst 분리 \n",
    "    (src, dst) = task_name.split(\"2\")\n",
    "    # 미리 저장된 embedding 경로\n",
    "    emb_dir = f\"assets/embeddings_sd_1.4\"\n",
    "    # embs_a, emb_b에 각각 해당되는 embedding을 불러옴. \n",
    "    embs_a = torch.load(os.path.join(emb_dir, f\"{src}.pt\"), map_location=device)\n",
    "    embs_b = torch.load(os.path.join(emb_dir, f\"{dst}.pt\"), map_location=device)\n",
    "    return (embs_b.mean(0)-embs_a.mean(0)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c741577-5dda-4ccf-a25d-a0375b83112d",
   "metadata": {},
   "source": [
    "## DDIM inversion\n",
    "\n",
    "실제 이미지를 초기 노이즈(latent)로 변환하는 Inversion 과정을 함수로 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56c10f-2aff-47c4-b2aa-db4bf38f752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inversion(input_image: str,\n",
    "                  results_folder: str,\n",
    "                  num_ddim_steps: int,\n",
    "                  model_path: str) -> (str, str, Image.Image, Image.Image):\n",
    "    \"\"\"\n",
    "    주어진 이미지에 대해 DDIM Inversion을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        input_image (str): 입력 이미지의 경로\n",
    "        results_folder (str): 결과가 저장될 폴더 경로\n",
    "        num_ddim_steps (int): DDIM Inversion 스텝 수\n",
    "        model_path (str): 사용할 Stable Diffusion 모델 경로\n",
    "\n",
    "    Returns:\n",
    "        tuple: (저장된 노이즈 경로, 저장된 프롬프트 경로, 원본 PIL 이미지, 복원된 PIL 이미지)\n",
    "    \"\"\"\n",
    "    # --- 이미지 다운로드 및 폴더 생성 ---\n",
    "    print(\"--- Inversion 시작 ---\")\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    os.makedirs(os.path.join(results_folder, \"inversion\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(results_folder, \"prompt\"), exist_ok=True)\n",
    "\n",
    "    # --- 1. 모델 로드 ---\n",
    "    print(\"[1/4] BLIP 및 DDIM Inversion 모델을 로드합니다...\")\n",
    "    model_blip, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device)\n",
    "    pipe = DDIMInversion.from_pretrained(model_path, torch_dtype=torch_dtype).to(device)\n",
    "    pipe.scheduler = DDIMInverseScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    # --- 2. 캡션 생성 ---\n",
    "    print(\"[2/4] 이미지 캡션을 생성합니다...\")\n",
    "    img = Image.open(input_image).resize((512, 512), Image.Resampling.LANCZOS)\n",
    "    _image = vis_processors[\"eval\"](img).unsqueeze(0).to(device)\n",
    "    prompt_str = model_blip.generate({\"image\": _image})[0]\n",
    "    print(f\"     생성된 캡션: '{prompt_str}'\")\n",
    "\n",
    "    # --- 3. DDIM Inversion 실행 ---\n",
    "    print(\"[3/4] DDIM Inversion을 실행하여 초기 노이즈를 찾습니다...\")\n",
    "    x_inv, _, x_dec_img = pipe(\n",
    "        prompt_str,\n",
    "        guidance_scale=1.0,\n",
    "        num_inversion_steps=num_ddim_steps,\n",
    "        img=img,\n",
    "        torch_dtype=torch_dtype\n",
    "    )\n",
    "\n",
    "    # --- 4. 결과 저장 ---\n",
    "    print(\"[4/4] Inversion 결과(노이즈)와 프롬프트를 저장합니다...\")\n",
    "    bname = os.path.basename(input_image).split(\".\")[0]\n",
    "    inverted_latent_path = os.path.join(results_folder, f\"inversion/{bname}.pt\")\n",
    "    prompt_path = os.path.join(results_folder, f\"prompt/{bname}.txt\")\n",
    "    torch.save(x_inv[0], inverted_latent_path)\n",
    "    with open(prompt_path, \"w\") as f:\n",
    "        f.write(prompt_str)\n",
    "    print(f\"   - 노이즈 저장 경로: {inverted_latent_path}\")\n",
    "    print(f\"   - 프롬프트 저장 경로: {prompt_path}\")\n",
    "    print(\"--- Inversion 완료 ---\\n\")\n",
    "\n",
    "    return inverted_latent_path, prompt_path, img, x_dec_img[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6f09d-9fc3-48a0-816f-bf329f591fad",
   "metadata": {},
   "source": [
    "## Edit image\n",
    "Inversion으로 얻은 노이즈와 프롬프트를 사용해 이미지를 편집하는 과정을 함수로 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f08ce-0a55-453e-9849-e864dff72e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_editing(inverted_latent_path: str,\n",
    "                prompt_path: str,\n",
    "                task_name: str,\n",
    "                edit_results_folder: str,\n",
    "                xa_guidance: float,\n",
    "                negative_guidance_scale: float,\n",
    "                num_ddim_steps: int,\n",
    "                model_path: str) -> (Image.Image, Image.Image):\n",
    "    \"\"\"\n",
    "    Inversion된 노이즈를 바탕으로 이미지를 편집합니다.\n",
    "\n",
    "    Args:\n",
    "        inverted_latent_path (str): 저장된 노이즈(.pt) 파일 경로\n",
    "        prompt_path (str): 저장된 프롬프트(.txt) 파일 경로\n",
    "        task_name (str): 편집 작업 이름 (예: 'cat2dog')\n",
    "        edit_results_folder (str): 편집된 이미지가 저장될 폴더\n",
    "        xa_guidance (float): 편집 방향 적용 강도\n",
    "        negative_guidance_scale (float): Negative guidance 강도\n",
    "        num_ddim_steps (int): DDIM 스텝 수\n",
    "        model_path (str): 사용할 Stable Diffusion 모델 경로\n",
    "\n",
    "    Returns:\n",
    "        tuple: (복원된 PIL 이미지, 편집된 PIL 이미지)\n",
    "    \"\"\"\n",
    "    # --- 폴더 생성 ---\n",
    "    print(\"--- Editing 시작 ---\")\n",
    "    os.makedirs(os.path.join(edit_results_folder, \"edit\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(edit_results_folder, \"reconstruction\"), exist_ok=True)\n",
    "\n",
    "    # --- 1. 편집 파이프라인 로드 ---\n",
    "    print(\"[1/4] 이미지 편집 파이프라인을 로드합니다...\")\n",
    "    pipe = EditingPipeline.from_pretrained(model_path, torch_dtype=torch_dtype).to(device)\n",
    "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    # --- 2. Inversion 결과 로드 ---\n",
    "    print(f\"[2/4] Inversion 결과를 로드합니다.\")\n",
    "    loaded_prompt_str = open(prompt_path).read().strip()\n",
    "    x_in = torch.load(inverted_latent_path).unsqueeze(0).to(device)\n",
    "\n",
    "    # --- 3. 편집 방향(Editing Direction) 설정 ---\n",
    "    print(f\"[3/4] '{task_name}' 작업에 대한 편집 방향을 설정합니다...\")\n",
    "    edit_dir = construct_direction(task_name)\n",
    "\n",
    "    # --- 4. 편집 실행 ---\n",
    "    print(\"[4/4] 이미지 편집을 실행합니다...\")\n",
    "    rec_pil, edit_pil = pipe(\n",
    "        loaded_prompt_str,\n",
    "        num_inference_steps=num_ddim_steps,\n",
    "        x_in=x_in,\n",
    "        edit_dir=edit_dir,\n",
    "        guidance_amount=xa_guidance,\n",
    "        guidance_scale=negative_guidance_scale,\n",
    "        negative_prompt=loaded_prompt_str\n",
    "    )\n",
    "\n",
    "    # --- 결과 저장 ---\n",
    "    bname = os.path.basename(inverted_latent_path).split(\".\")[0]\n",
    "    edited_image_path = os.path.join(edit_results_folder, f\"edit/{bname}_{task_name}.png\")\n",
    "    recon_image_path = os.path.join(edit_results_folder, f\"reconstruction/{bname}.png\")\n",
    "    edit_pil[0].save(edited_image_path)\n",
    "    rec_pil[0].save(recon_image_path)\n",
    "    print(f\"결과가 다음 경로에 저장되었습니다:\\n   - 편집 이미지: {edited_image_path}\")\n",
    "    print(\"--- Editing 완료 ---\\n\")\n",
    "\n",
    "    return rec_pil[0], edit_pil[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19eb5d2-39ee-4262-80ce-dee754ea612f",
   "metadata": {},
   "source": [
    "## Pipeline 실행 및 결과 확인\n",
    "위에서 정의된 함수들을 호출하여 전체 과정을 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a4aba-91ff-4cbf-80f2-8fb45b355d40",
   "metadata": {},
   "source": [
    "### Inversion 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536bea6-c58c-4460-93a9-499a653474ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common parameters\n",
    "model_path = 'CompVis/stable-diffusion-v1-4' \n",
    "num_ddim_steps = 50\n",
    "\n",
    "# Inversion parameters\n",
    "input_image = 'assets/test_images/cats/cat_1.png' \n",
    "inversion_folder = 'output/cat_test' \n",
    "\n",
    "\n",
    "inv_latent_path, inv_prompt_path, original_image, reconstructed_image = run_inversion(\n",
    "    input_image=input_image,\n",
    "    results_folder=inversion_folder,\n",
    "    num_ddim_steps=num_ddim_steps,\n",
    "    model_path=model_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74146b15-3fc7-456b-83c4-c5d8986bcff9",
   "metadata": {},
   "source": [
    "### Editing 함수 호출 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ade9fa-419d-4f06-9ddb-b5a29bb25874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editing parameters\n",
    "# (예: `cat2dog`, `horse2zebra`, `orange2apple`, `gold2wood`, `cat_statue`)\n",
    "task_name = 'cat2dog'\n",
    "editing_folder = 'output/test_cat' \n",
    "xa_guidance = 0.1\n",
    "negative_guidance_scale = 5.0\n",
    "\n",
    "# edit\n",
    "final_reconstructed_image, edited_image = run_editing(\n",
    "    inverted_latent_path=inv_latent_path,\n",
    "    prompt_path=inv_prompt_path,\n",
    "    task_name=task_name,\n",
    "    edit_results_folder=editing_folder,\n",
    "    xa_guidance=xa_guidance,\n",
    "    negative_guidance_scale=negative_guidance_scale,\n",
    "    num_ddim_steps=num_ddim_steps,\n",
    "    model_path=model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb444c87-cec4-4327-b686-c256f64ba255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(final_reconstructed_image)\n",
    "axes[1].set_title(\"Reconstructed Image\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(edited_image)\n",
    "axes[2].set_title(f\"Edited Image ('{task_name}')\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
