{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70657d36-7f9d-41b7-985f-51b171952c70",
   "metadata": {},
   "source": [
    "# Classifier-free Diffusion Guidance\n",
    "\n",
    "\n",
    "Let's build and train a diffusion model with classifier-free guidance on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532e135-aefb-4ccd-b042-32e63f2361e7",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "$\\displaystyle \\tilde{\\epsilon}_{\\theta}(\\mathbf{x}_t, y) = (1 + s) \\epsilon_{\\theta}(\\mathbf{x}_t, y) - s \\epsilon_{\\theta}(\\mathbf{x}_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443bb99-5543-44f0-9134-03fe05df0590",
   "metadata": {},
   "source": [
    "## Build and Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61cfff-0b40-4484-b3c6-2d50f4e26a79",
   "metadata": {},
   "source": [
    "Before we start, we need to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759dce3c-937d-41f9-8d32-14f508e4bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision numpy matplotlib tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "import sys\n",
    "folder_path = '/content/drive/MyDrive/Colab/'\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Working on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580075c8-e708-4272-a57f-e80d30ca8e36",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b2294-9142-4806-870f-f825cc14a74a",
   "metadata": {},
   "source": [
    "We load images in CIFAR-10 dataset and create the dataloader.<br>\n",
    "Each batch has shape ```[batch_size, 3, 32 32]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f72009-e6a2-48ca-b107-2534916dbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "dataset = datasets.CIFAR10(\n",
    "    \"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ee6fd-94d1-43f8-be42-7b90d6f00f19",
   "metadata": {},
   "source": [
    "In this dataset, there exist 10 classes as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcda87-d4aa-4477-aa5a-b0be3ac3269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2852ed2-4141-4839-af75-5c99094d05cd",
   "metadata": {},
   "source": [
    "Now, I will show 100 examples. (10 examples per class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9dbc9-68fe-48fa-b320-d8c89c689d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images_by_label = [[] for _ in range(10)]\n",
    "num_images_per_label = 10\n",
    "\n",
    "# Collect 10 examples for each class\n",
    "for image, label in dataset:\n",
    "  \n",
    "    if len(images_by_label[label]) < num_images_per_label:\n",
    "        images_by_label[label].append(image)\n",
    "    \n",
    "    # Check if 10 images are collected for each class\n",
    "    is_fully_collected = all(len(lst) == num_images_per_label for lst in images_by_label)\n",
    "    if is_fully_collected:\n",
    "        break\n",
    "\n",
    "num_row = 10\n",
    "num_col = 10\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(10, 10))\n",
    "fig.suptitle('CIFAR-10: 10 Images per Label (0-9)', fontsize=16)\n",
    "\n",
    "for label_idx in range(num_row):\n",
    "    for img_idx in range(num_col):\n",
    "        image_tensor = images_by_label[label_idx][img_idx]\n",
    "        image_numpy = image_tensor.numpy()\n",
    "        \n",
    "        channel_last = np.transpose(image_numpy, (1, 2, 0))\n",
    "        \n",
    "        ax = axes[label_idx, img_idx]\n",
    "        ax.imshow(channel_last)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        if img_idx == 0:\n",
    "            ax.set_ylabel(f'Label: {label_idx}', rotation=0, labelpad=30, fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af671b-f6e8-4208-9c26-c595a441e504",
   "metadata": {},
   "source": [
    "### 2. Build class conditional U-Net $\\epsilon_{\\theta}(\\mathbf{x}, y)$\n",
    "\n",
    "The following code (model implementation) is mostly the same as DDPM U-Net, but the class embedding layer is separated from U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2abc26-647d-4baf-ad5c-b101e2564e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591bec1-617f-4a1f-8bcf-d327db8a39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define blocks\n",
    "#\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_dim, # 128\n",
    "        hidden_dim, # 256\n",
    "        output_dim, # 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # In this example, we assume that the number of embedding dimension is always even.\n",
    "        # (If not, please pad the result.)\n",
    "        assert(base_dim % 2 == 0)\n",
    "        self.timestep_dim = base_dim\n",
    "\n",
    "        self.hidden1 = nn.Linear(\n",
    "            base_dim,\n",
    "            hidden_dim)\n",
    "        self.hidden2 = nn.Linear(\n",
    "            hidden_dim,\n",
    "            output_dim)\n",
    "\n",
    "    def forward(self, picked_up_timesteps):\n",
    "        # Generate 1 / 10000^{2i / d_e}\n",
    "        # shape : (timestep_dim / 2, )\n",
    "        interval = 1.0 / (10000**(torch.arange(0, self.timestep_dim, 2.0).to(device) / self.timestep_dim))\n",
    "        # Generate t / 10000^{2i / d_e}\n",
    "        # shape : (batch_size, timestep_dim / 2)\n",
    "        position = picked_up_timesteps.type(torch.get_default_dtype())\n",
    "        radian = position[:, None] * interval[None, :]\n",
    "        # Get sin(t / 10000^{2i / d_e}) and unsqueeze\n",
    "        # shape : (batch_size, timestep_dim / 2, 1)\n",
    "        sin = torch.sin(radian).unsqueeze(dim=-1)\n",
    "        # Get cos(t / 10000^{2i / d_e}) and unsqueeze\n",
    "        # shape : (batch_size, timestep_dim / 2, 1)\n",
    "        cos = torch.cos(radian).unsqueeze(dim=-1)\n",
    "        # Get sinusoidal positional encoding\n",
    "        # shape : (batch_size, timestep_dim)\n",
    "        pe_tmp = torch.concat((sin, cos), dim=-1)   # shape : (num_timestep, timestep_dim / 2, 2)\n",
    "        d = pe_tmp.size()[1]\n",
    "        pe = pe_tmp.view(-1, d * 2)                 # shape : (num_timestep, timestep_dim)\n",
    "        # Apply feedforward\n",
    "        # shape : (batch_size, timestep_dim * 4)\n",
    "        out = self.hidden1(pe)\n",
    "        out = F.silu(out)\n",
    "        out = self.hidden2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        num_norm_groups, # 32\n",
    "        embedding_dim, # 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # for normalization\n",
    "        self.norm1 = nn.GroupNorm(\n",
    "            num_groups=num_norm_groups,\n",
    "            num_channels=in_channel,\n",
    "            eps=1e-06,\n",
    "        )\n",
    "        self.norm2 = nn.GroupNorm(\n",
    "            num_groups=num_norm_groups,\n",
    "            num_channels=out_channel,\n",
    "            eps=1e-06,\n",
    "        )\n",
    "\n",
    "        # for applying conv\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channel,\n",
    "            out_channel,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channel,\n",
    "            out_channel,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "\n",
    "        # for time and class projection\n",
    "        self.linear_time = nn.Linear(embedding_dim, out_channel)\n",
    "        self.linear_class = nn.Linear(embedding_dim, out_channel)\n",
    "\n",
    "        # for residual block\n",
    "        if in_channel != out_channel:\n",
    "            self.linear_src = nn.Linear(in_channel, out_channel)\n",
    "        else:\n",
    "            self.linear_src = None\n",
    "\n",
    "    def forward(self, x, t_emb, y_emb):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.tensor((batch_size, in_channel, width, height), dtype=float)\n",
    "            input x\n",
    "        t_emb : torch.tensor((batch_size, base_channel_dim * 4), dtype=float)\n",
    "            timestep embeddings\n",
    "        y_emb : torch.tensor((batch_size, base_channel_dim * 4), dtype=float)\n",
    "            class embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        # apply conv\n",
    "        out = self.norm1(x)\n",
    "        out = F.silu(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        # apply AdaGN (adaptive group normalization)\n",
    "        t_prj = F.silu(t_emb)\n",
    "        t_prj = self.linear_time(t_prj)\n",
    "        t_prj = t_prj[:, :, None, None]\n",
    "\n",
    "        y_prj = F.silu(y_emb)\n",
    "        y_prj = self.linear_class(y_prj)\n",
    "        y_prj = y_prj[:, :, None, None]\n",
    "\n",
    "        out = out * t_prj + y_prj\n",
    "\n",
    "        # apply dropout + conv\n",
    "        out = self.norm2(out)\n",
    "        out = F.silu(out)\n",
    "        out = F.dropout(out, p=0.1, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # apply residual\n",
    "        if self.linear_src is not None:\n",
    "            x_trans = x.permute(0, 2, 3, 1)       # (N,C,H,W) --> (N,H,W,C)\n",
    "            x_trans = self.linear_src(x_trans)\n",
    "            x_trans = x_trans.permute(0, 3, 1, 2) # (N,H,W,C) --> (N,C,H,W)\n",
    "            out = out + x_trans\n",
    "        else:\n",
    "            out = out + x\n",
    "\n",
    "        return out\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channel,\n",
    "        num_norm_groups, # 32\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.GroupNorm(\n",
    "            num_groups=num_norm_groups,\n",
    "            num_channels=channel,\n",
    "            eps=1e-06,\n",
    "        )\n",
    "\n",
    "        self.q_layer = nn.Linear(channel, channel)\n",
    "        self.k_layer = nn.Linear(channel, channel)\n",
    "        self.v_layer = nn.Linear(channel, channel)\n",
    "\n",
    "        self.output_linear = nn.Linear(channel, channel, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel = x.size(dim=1)\n",
    "        height = x.size(dim=2)\n",
    "        width = x.size(dim=3)\n",
    "\n",
    "        out = self.norm(x)\n",
    "\n",
    "        # reshape : (N,C,H,W) --> (N,H*W,C)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        out = out.view(-1, height*width, channel)\n",
    "\n",
    "        # generate query/key/value\n",
    "        q = self.q_layer(out)\n",
    "        k = self.k_layer(out)\n",
    "        v = self.v_layer(out)\n",
    "\n",
    "        # compute Q K^T\n",
    "        score = torch.einsum(\"bic,bjc->bij\", q, k)\n",
    "\n",
    "        # scale the result by 1/sqrt(channel)\n",
    "        score = score / channel**0.5\n",
    "\n",
    "        # apply softtmax\n",
    "        score = F.softmax(score, dim=-1)\n",
    "\n",
    "        # apply dot product with values\n",
    "        out = torch.einsum(\"bij,bjc->bic\", score, v)\n",
    "\n",
    "        # apply final linear\n",
    "        out = self.output_linear(out)\n",
    "\n",
    "        # reshape : (N,H*W,C) --> (N,C,H,W)\n",
    "        out = out.view(-1, height, width, channel)\n",
    "        out = out.permute(0, 3, 1, 2)\n",
    "\n",
    "        # apply residual\n",
    "        out = out + x\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResnetAndAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        num_norm_groups, # 32\n",
    "        embedding_dim, # 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resnet = ResnetBlock(\n",
    "            in_channel,\n",
    "            out_channel,\n",
    "            num_norm_groups,\n",
    "            embedding_dim,\n",
    "        )\n",
    "        self.attention = AttentionBlock(\n",
    "            out_channel,\n",
    "            num_norm_groups,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb, y_emb):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.tensor((batch_size, in_channel, width, height), dtype=float)\n",
    "            input x\n",
    "        t_emb : torch.tensor((batch_size, base_channel_dim * 4), dtype=float)\n",
    "            timestep embeddings\n",
    "        y_emb : torch.tensor((batch_size, base_channel_dim * 4), dtype=float)\n",
    "            class embeddings\n",
    "        \"\"\"\n",
    "        out = self.resnet(x, t_emb, y_emb)\n",
    "        out = self.attention(out)\n",
    "        return out\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channel,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            channel,\n",
    "            channel,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channel,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            channel,\n",
    "            channel,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        out = self.conv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155348cf-d084-4bd8-b76d-84840ccc717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define U-Net\n",
    "#\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_channel, # 3\n",
    "        unet_base_channel, # 128\n",
    "        num_norm_groups, # 32\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_enc = PositionalEncoding(\n",
    "            base_dim=unet_base_channel,\n",
    "            hidden_dim=unet_base_channel*2,\n",
    "            output_dim=unet_base_channel*4,\n",
    "        )\n",
    "\n",
    "        self.down_conv = nn.Conv2d(\n",
    "            source_channel,\n",
    "            unet_base_channel,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.top_to_down = nn.ModuleList([\n",
    "            # 1st layer\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel,\n",
    "                out_channel=unet_base_channel,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel,\n",
    "                out_channel=unet_base_channel,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            DownSample(\n",
    "                channel=unet_base_channel,\n",
    "            ),\n",
    "            # 2nd layer\n",
    "            ResnetAndAttention(\n",
    "                in_channel=unet_base_channel,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetAndAttention(\n",
    "                in_channel=unet_base_channel*2,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            DownSample(\n",
    "                channel=unet_base_channel*2,\n",
    "            ),\n",
    "            # 3rd layer\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*2,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*2,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            DownSample(\n",
    "                channel=unet_base_channel*2,\n",
    "            ),\n",
    "            # 4th layer\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*2,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*2,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "        ])\n",
    "        self.middle = nn.ModuleList([\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*2,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            AttentionBlock(\n",
    "                channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "            ),\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*2,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "        ])\n",
    "        self.bottom_to_up = nn.ModuleList([\n",
    "            # 1st layer\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*4,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*4,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*4,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            UpSample(\n",
    "                channel=unet_base_channel*2,\n",
    "            ),\n",
    "            # 2nd layer\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*4,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*4,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*4,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            UpSample(\n",
    "                channel=unet_base_channel*2,\n",
    "            ),\n",
    "            # 3rd layer\n",
    "            ResnetAndAttention(\n",
    "                in_channel=unet_base_channel*4,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetAndAttention(\n",
    "                in_channel=unet_base_channel*4,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetAndAttention(\n",
    "                in_channel=unet_base_channel*3,\n",
    "                out_channel=unet_base_channel*2,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            UpSample(\n",
    "                channel=unet_base_channel*2,\n",
    "            ),\n",
    "            # 4th layer\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*3,\n",
    "                out_channel=unet_base_channel,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*2,\n",
    "                out_channel=unet_base_channel,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "            ResnetBlock(\n",
    "                in_channel=unet_base_channel*2,\n",
    "                out_channel=unet_base_channel,\n",
    "                num_norm_groups=num_norm_groups,\n",
    "                embedding_dim=unet_base_channel*4,\n",
    "            ),\n",
    "        ])\n",
    "        self.norm = nn.GroupNorm(\n",
    "            num_groups=num_norm_groups,\n",
    "            num_channels=unet_base_channel,\n",
    "            eps=1e-06,\n",
    "        )\n",
    "        self.up_conv = nn.Conv2d(\n",
    "            unet_base_channel,\n",
    "            source_channel,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, y_emb):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.tensor((batch_size, in_channel, width, height), dtype=float)\n",
    "            Gaussian-noised images\n",
    "        t : torch.tensor((batch_size), dtype=int)\n",
    "            timestep\n",
    "        y_emb : torch.tensor((batch_size, base_channel_dim * 4), dtype=float)\n",
    "            class embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        buffer = []\n",
    "\n",
    "        # generate time embedding\n",
    "        time_embs = self.pos_enc(t)\n",
    "\n",
    "        #\n",
    "        # Top-to-down\n",
    "        #\n",
    "\n",
    "        # apply down-convolution\n",
    "        out = self.down_conv(x)\n",
    "        buffer.append(out)\n",
    "        # apply top-to-down\n",
    "        for block in self.top_to_down:\n",
    "            if isinstance(block, ResnetBlock):\n",
    "                out = block(out, time_embs, y_emb)\n",
    "            elif isinstance(block, ResnetAndAttention):\n",
    "                out = block(out, time_embs, y_emb)\n",
    "            elif isinstance(block, DownSample):\n",
    "                out = block(out)\n",
    "            else:\n",
    "                raise Exception(\"Unknown block\")\n",
    "            buffer.append(out)\n",
    "\n",
    "        #\n",
    "        # Middle\n",
    "        #\n",
    "        for block in self.middle:\n",
    "            if isinstance(block, ResnetBlock):\n",
    "                out = block(out, time_embs, y_emb)\n",
    "            elif isinstance(block, AttentionBlock):\n",
    "                out = block(out)\n",
    "            else:\n",
    "                raise Exception(\"Unknown block\")\n",
    "\n",
    "        #\n",
    "        # Bottom-to-up\n",
    "        #\n",
    "\n",
    "        # apply bottom-to-up\n",
    "        for block in self.bottom_to_up:\n",
    "            if isinstance(block, ResnetBlock):\n",
    "                buf = buffer.pop()\n",
    "                out = torch.cat((out, buf), dim=1)\n",
    "                out = block(out, time_embs, y_emb)\n",
    "            elif isinstance(block, ResnetAndAttention):\n",
    "                buf = buffer.pop()\n",
    "                out = torch.cat((out, buf), dim=1)\n",
    "                out = block(out, time_embs, y_emb)\n",
    "            elif isinstance(block, UpSample):\n",
    "                out = block(out)\n",
    "            else:\n",
    "                raise Exception(\"Unknown block\")\n",
    "        # apply up-convolution\n",
    "        out = self.norm(out)\n",
    "        out = F.silu(out)\n",
    "        out = self.up_conv(out)\n",
    "\n",
    "        assert not buffer\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f06e0-5227-4280-8eaa-740ea92d3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate model (U-Net)\n",
    "#\n",
    "unet_base_channel = 128\n",
    "emb = nn.Embedding(num_classes, unet_base_channel*4).to(device)\n",
    "unet = UNet(\n",
    "    source_channel=3,\n",
    "    unet_base_channel=unet_base_channel,\n",
    "    num_norm_groups=32,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6486369-8d4c-44de-8b3f-93e3d0def4dd",
   "metadata": {},
   "source": [
    "### 3. Jointly train $\\epsilon_{\\theta}(\\mathbf{x}, y)$ and $\\epsilon_{\\theta}(\\mathbf{x}, \\varnothing)$\n",
    "\n",
    "Now let's train $\\epsilon_{\\theta}$.\n",
    "\n",
    "We needs 2 score functions - a conditional score $\\epsilon_{\\theta}(\\mathbf{x}, y)$ and an unconditional $\\epsilon_{\\theta}(\\mathbf{x})$, but we jointly trains these 2 scores with a single neural network.\n",
    "\n",
    "For the initial setup, we first set the probability of unconditional training, $p_{\\verb|uncond|}$. (Here we set $p_{\\verb|uncond|} = 0.2$.)<br>\n",
    "This means, during the training, we set $y \\gets \\varnothing$ with probability $p_{\\verb|uncond|}$.\n",
    "We use the same noise scheduling as DDPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e1349-7a3b-40f7-aed9-cc3635dbc10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "p_uncond = 0.2\n",
    "\n",
    "# initialize optimizer\n",
    "opt = torch.optim.Adam(list(emb.parameters()) + list(unet.parameters()), lr=2e-4, eps=1e-08)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    opt,\n",
    "    start_factor=1.0/5000,\n",
    "    end_factor=1.0,\n",
    "    total_iters=5000)\n",
    "\n",
    "# 1. Initialize T and alpha\n",
    "#   (Use double type for precision.)\n",
    "T = 1000\n",
    "alphas = #FILL\n",
    "alpha_bars = #FILL # use torch.cumprod\n",
    "sqrt_alpha_bars_t = #FILL # use torch.sqrt\n",
    "sqrt_one_minus_alpha_bars_t = #FILL \n",
    "\n",
    "# remove log file if exists\n",
    "log_file = \"train_loss.log\"\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# loop\n",
    "num_epochs = 2\n",
    "for epoch_idx in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (data, y) in enumerate(loader):\n",
    "        unet.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Pick up x_0 (shape: [64, 3, 32, 32])\n",
    "        x_0 = data.to(device)\n",
    "        y_ = y.to(device)\n",
    "\n",
    "        # Pick up a random timestep, t.\n",
    "        #    Instead of picking up t=1,2, ... ,T ,\n",
    "        #    here we pick up t=0,1, ... ,T-1 .\n",
    "        #   (i.e, t == 0 means diffused for 1 step)\n",
    "        b = x_0.size(dim=0)\n",
    "        t = torch.randint(T, (b,)).to(device)\n",
    "\n",
    "        # Generate the seed of noise, epsilon.\n",
    "        #    We just pick up from 1D standard normal distribution with the same shape,\n",
    "        #    because off-diagonal elements in the covariance are all zero.\n",
    "        eps = torch.randn_like(x_0).to(device)\n",
    "\n",
    "        # 2. Get x_t and y embedding\n",
    "        # Compute x_t = sqrt(alpha_bar_t) x_0 + sqrt(1-alpha_bar_t) epsilon\n",
    "        #    (t == 0 means diffused for 1 step)\n",
    "        x_t = #FILL\n",
    "        \n",
    "        # Get class embedding\n",
    "        y_emb = #FILL\n",
    "\n",
    "        # 3. Set empty in class embedding with probability p_uncond (see above)\n",
    "        rnd = #FILL \n",
    "        mul = #FILL use torch.where\n",
    "        y_emb = #FILL\n",
    "\n",
    "        # 4. Get loss and apply gradient (update)\n",
    "        model_out = #FILL \n",
    "        loss = #FILL use mse loss (recap DDPM tutorial!)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # log\n",
    "        epoch_loss.append(loss.item())\n",
    "        print(\"epoch{} (iter{}) - loss {:5.4f}\".format(epoch_idx+1, batch_idx+1, loss), end=\"\\r\")\n",
    "\n",
    "    # finalize epoch (save log and checkpoint)\n",
    "    epoch_average_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    print(\"epoch{} (iter{}) - loss {:5.4f}\".format(epoch_idx+1, batch_idx+1, epoch_average_loss))\n",
    "    with open(log_file, \"a\") as f:\n",
    "        for l in epoch_loss:\n",
    "            f.write(\"%s\\n\" %l)\n",
    "    torch.save(unet.state_dict(), f\"guided_unet_{epoch_idx+1}.pt\")\n",
    "    torch.save(emb.state_dict(), f\"guided_embedding_{epoch_idx+1}.pt\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70309f96-fc2c-4540-b5f4-f589e285d204",
   "metadata": {},
   "source": [
    "## Classifier-free guided image generation\n",
    "\n",
    "As we saw above, here we perform sampling with the following noise prediction update. We want to generate a grid of images given a class.\n",
    "\n",
    "$\\displaystyle \\tilde{\\epsilon}_{\\theta}(\\mathbf{x}_t, y) = (1 + s) \\epsilon_{\\theta}(\\mathbf{x}_t, y) - s \\epsilon_{\\theta}(\\mathbf{x}_t, \\varnothing)$\n",
    "\n",
    "As you can see below results, the diversity increases when you reduce the scale $s$, but large $s$ improves sampling fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7361923-cc47-4a7b-ab22-13193e014d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def run_inference(unet, emb, class_name, class_list, s, num_row=10, num_col=10):\n",
    "    unet.eval()\n",
    "\n",
    "    ##########\n",
    "    # 5. generate sigma_t\n",
    "    ##########\n",
    "    alpha_bars_prev = torch.cat((torch.ones(1).to(device), alpha_bars[:-1]))\n",
    "    sigma_t_squared = #FILL\n",
    "    sigma_t = #FILL\n",
    "\n",
    "    ##########\n",
    "    # make white noise\n",
    "    ##########\n",
    "    x = torch.randn(num_row*num_col, 3, 32, 32).to(device)\n",
    "\n",
    "    ##########\n",
    "    # 6. generate images\n",
    "    ##########\n",
    "    with torch.no_grad():\n",
    "        # generate class embedding\n",
    "        # (the first half is for epsilon(x, y), the second half is for epsilon(x, empty))\n",
    "        class_id_list = [i for i,v in enumerate(class_list) if v==class_name]\n",
    "        if len(class_id_list) == 0:\n",
    "            raise Exception(\"class name doesn't exist\")\n",
    "        y = class_id_list[0]\n",
    "        y_batch = (torch.tensor(y).to(device)).repeat(num_row*num_col)\n",
    "        y_batch = torch.cat((y_batch, y_batch), dim=0)\n",
    "        y_emb_batch = emb(y_batch)\n",
    "        mask = # FILL use torch.cat, torch.ones, torch.zeros\n",
    "        y_emb_batch = y_emb_batch * mask[:,None]\n",
    "        # loop T-1, T-2, ... ,0\n",
    "        for t in tqdm.tqdm(reversed(range(T)), total=T):\n",
    "            # generate t\n",
    "            # (the first half is for epsilon(x, y), the second half is for epsilon(x, empty))\n",
    "            t_batch = (torch.tensor(t).to(device)).repeat(num_row*num_col)\n",
    "            t_batch = torch.cat((t_batch, t_batch), dim=0)\n",
    "            # compute epsilon\n",
    "            # (the first half is for epsilon(x, y), the second half is for epsilon(x, empty))\n",
    "            x_batch = #FILL see t_batch \n",
    "            eps_batch = #FILL feed model \n",
    "            eps_cond, eps_uncond = #FILL use torch.split\n",
    "            eps = #FILL apply CFG method\n",
    "            # update x\n",
    "            if t > 0:\n",
    "                z = torch.randn(num_row*num_col, 3, 32, 32).to(device)\n",
    "            else:\n",
    "                z = torch.zeros(num_row*num_col, 3, 32, 32).to(device)\n",
    "            x = #FILL use DDPM sampling algorithm\n",
    "\n",
    "    ##########\n",
    "    # output\n",
    "    ##########\n",
    "\n",
    "    # reshape to channels-last : (N,C,H,W) --> (N,H,W,C)    \n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    # clip\n",
    "    x = torch.clamp(x, min=0.0, max=1.0)\n",
    "    # draw\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(5,5))\n",
    "    for i in range(num_row*num_col):\n",
    "        image = x[i].cpu().numpy()\n",
    "        row = i//num_col\n",
    "        col = i%num_col\n",
    "        ax = axes[row, col]\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3547c685-3960-41ed-9ef1-5bb045492544",
   "metadata": {},
   "source": [
    "### Load pretrained model\n",
    "Since we don't have enough time to finish training, let's load the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293047d-15f4-420f-aff5-7cad09d93353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Load pretrained model\n",
    "unet_path = f\"/content/drive/MyDrive/Colab/guided_unet_200.pt\"\n",
    "emb_path = f\"/content/drive/MyDrive/Colab/guided_embedding_200.pt\"\n",
    "\n",
    "# state_dict\n",
    "unet_state_dict = torch.load(unet_path, map_location=torch.device(device))\n",
    "emb_state_dict = torch.load(emb_path, map_location=torch.device(device))\n",
    "\n",
    "# load model\n",
    "unet.load_state_dict(unet_state_dict)\n",
    "emb.load_state_dict(emb_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb92b6c-c25d-47e0-aa74-f5ddd043a866",
   "metadata": {},
   "source": [
    "#### $s=3.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1e8fb-cd35-410a-a4f0-44e0c2fde47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    unet,\n",
    "    emb,\n",
    "    class_name=\"automobile\",\n",
    "    class_list=classes,\n",
    "    s=3.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd5389-97cd-4343-95e9-5c751f8a4901",
   "metadata": {},
   "source": [
    "#### $s=0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194ade4-9b25-4f0b-bb5a-f5f855482e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    unet,\n",
    "    emb,\n",
    "    class_name=\"automobile\",\n",
    "    class_list=classes,\n",
    "    s=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7a2a9-bdd2-4a1c-98cc-e9c666e644f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
